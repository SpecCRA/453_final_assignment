{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSDS Assignment 4: Sentiment Analysis of Amazon Reviews\n",
    "## Category: Video games\n",
    "## Author: Ben Xiao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Setup #######\n",
    "# Import packages\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re, string\n",
    "import nltk\n",
    "import gensim\n",
    "import multiprocessing\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import support libraries\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Embedding, Input\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Check package version:\nPython: 3.8.2 (default, Apr 27 2020, 15:53:34) \n[GCC 9.3.0]\npandas: 1.0.3\nNumPy: 1.18.3\nReGex: 2.2.1\nscikit-learn: 0.22.2.post1\nmatplotlib: 3.2.1\n"
    }
   ],
   "source": [
    "# Check package versions\n",
    "print('Check package version:')\n",
    "print('Python: {}'.format(sys.version))\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('NumPy: {}'.format(np.__version__))\n",
    "print('ReGex: {}'.format(re.__version__))\n",
    "print('scikit-learn: {}'.format(sklearn.__version__))\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text parsing settings\n",
    "STEMMING = True\n",
    "seed = 88\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   overall                                         reviewText        asin  \\\n0        1  I used to play this game years ago and loved i...  0439381673   \n1        3  The game itself worked great but the story lin...  0439381673   \n2        4  I had to learn the hard way after ordering thi...  0439381673   \n3        1  The product description should state this clea...  0439381673   \n4        4  I would recommend this learning game for anyon...  0439381673   \n\n   word_counts  \n0          139  \n1          145  \n2          447  \n3          157  \n4          120  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>overall</th>\n      <th>reviewText</th>\n      <th>asin</th>\n      <th>word_counts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>I used to play this game years ago and loved i...</td>\n      <td>0439381673</td>\n      <td>139</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>The game itself worked great but the story lin...</td>\n      <td>0439381673</td>\n      <td>145</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>I had to learn the hard way after ordering thi...</td>\n      <td>0439381673</td>\n      <td>447</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>The product description should state this clea...</td>\n      <td>0439381673</td>\n      <td>157</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>I would recommend this learning game for anyon...</td>\n      <td>0439381673</td>\n      <td>120</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Import and check data\n",
    "data_path = 'data_files/short_reviews.pkl'\n",
    "df = pd.read_pickle(data_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# DEVELOPMENT SET - comment out when done\n",
    "############################\n",
    "df = df.iloc[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allocate train/validate/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, validate, test\n",
    "fracs = np.array([0.6, 0.2, 0.2])\n",
    "\n",
    "# shuffle dataframe rows before splitting\n",
    "df = df.sample(frac=1, random_state=seed)\n",
    "\n",
    "# split into 3 parts\n",
    "train, val, test = np.array_split(\n",
    "    df, (fracs[:-1].cumsum() * len(df)).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_doc(doc):\n",
    "    # split document into individual words\n",
    "    tokens=doc.split()\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 4]\n",
    "    # filter out tokens more than 20 characters long\n",
    "    tokens = [word for word in tokens if len(word) < 21]\n",
    "    #lowercase all words\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # word stemming Commented\n",
    "    if STEMMING:\n",
    "        ps=PorterStemmer()\n",
    "        tokens=[ps.stem(word) for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to record time\n",
    "def parse_time(start_time, end_time):\n",
    "    runtime = end_time - start_time\n",
    "    return round(runtime, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate documents\n",
    "train_docs = list()\n",
    "val_docs = list()\n",
    "test_docs = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Start cleaning training docs...\nFinished cleaning training docs...\nCleaning runtime: 0.6\nStart cleaning validation docs...\nFinished cleaning validation docs...\nCleaning runtime: 0.193\nStart cleaning test docs...\nFinished cleaning test docs...\nCleaning runtime: 0.196\n"
    }
   ],
   "source": [
    "# Clean training set\n",
    "print('Start cleaning training docs...')\n",
    "start_clean = time.time()\n",
    "for i in range(len(train)):\n",
    "    temp_text = df.reviewText.iloc[i]\n",
    "    cleaned_doc = clean_doc(temp_text)\n",
    "\n",
    "    combined_text = ' '.join(cleaned_doc)\n",
    "    train_docs.append(combined_text)\n",
    "\n",
    "end_clean = time.time()\n",
    "print('Finished cleaning training docs...')\n",
    "clean_runtime = parse_time(start_clean, end_clean)\n",
    "print('Cleaning runtime: {}'.format(clean_runtime))\n",
    "#print(docs[0])\n",
    "\n",
    "# Clean validation set\n",
    "print('Start cleaning validation docs...')\n",
    "start_clean = time.time()\n",
    "for i in range(len(val)):\n",
    "    temp_text = df.reviewText.iloc[i]\n",
    "    cleaned_doc = clean_doc(temp_text)\n",
    "    combined_text = ' '.join(cleaned_doc)\n",
    "    val_docs.append(combined_text)\n",
    "\n",
    "end_clean = time.time()\n",
    "print('Finished cleaning validation docs...')\n",
    "clean_runtime = parse_time(start_clean, end_clean)\n",
    "print('Cleaning runtime: {}'.format(clean_runtime))\n",
    "\n",
    "# Clean test set\n",
    "print('Start cleaning test docs...')\n",
    "start_clean = time.time()\n",
    "for i in range(len(val)):\n",
    "    temp_text = df.reviewText.iloc[i]\n",
    "    cleaned_doc = clean_doc(temp_text)\n",
    "    combined_text = ' '.join(cleaned_doc)\n",
    "    test_docs.append(combined_text)\n",
    "\n",
    "end_clean = time.time()\n",
    "print('Finished cleaning test docs...')\n",
    "clean_runtime = parse_time(start_clean, end_clean)\n",
    "print('Cleaning runtime: {}'.format(clean_runtime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF - 100 dimension, 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 100\n",
    "n_grams = 2\n",
    "\n",
    "# 1. TF-IDF\n",
    "tfidf100 = TfidfVectorizer(ngram_range=(1, n_grams),\n",
    "                                max_features=ndim)\n",
    "tfidf100_train_matrix = tfidf100.fit_transform(train_docs)\n",
    "tfidf100_val_matrix = tfidf100.transform(val_docs)\n",
    "tfidf100_test_matrx = tfidf100.transform(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{k: v for k, v in sorted(tfidf_100.vocabulary_.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF 200 dimension, 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf200 = TfidfVectorizer(max_features=200) # default n-gram is 1\n",
    "tfidf200_train_matrix = tfidf200.fit_transform(train_docs)\n",
    "tfidf200_val_matrix = tfidf200.transform(val_docs)\n",
    "tfidf200_test_matrix = tfidf200.transform(test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe - 100 dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 400000 word vectors.\n"
    }
   ],
   "source": [
    "glove100_path = 'glove.6B/glove.6B.100d.txt'\n",
    "ndim = 100\n",
    "glove100_embeddings_index = {}\n",
    "with open(glove100_path) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        glove100_embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(glove100_embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list()\n",
    "tokenizer = Tokenizer(nb_words = 100)\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "sequences = tokenizer.texts_to_sequences(train_docs)\n",
    "data = pad_sequences(sequences, maxlen=100)\n",
    "word_index = tokenizer.word_index\n",
    "len(word_index)\n",
    "\n",
    "glove100_test = tokenizer.texts_to_sequences(test_docs)\n",
    "glove100_val = tokenizer.texts_to_sequences(val_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required or to_categorical creates 6 labels\n",
    "#labels = np.asarray(df.overall.apply(lambda x: x-1))\n",
    "glove_train_labels = np.asarray(train.overall.apply(lambda x: x-1))\n",
    "glove_val_labels = np.asarray(val.overall.apply(lambda x: x-1))\n",
    "glove_test_labels = np.asarray(test.overall.apply(lambda x: x-1))\n",
    "\n",
    "\n",
    "glove_train_labels = to_categorical(glove_train_labels, num_classes=5) # number of classes, 1-5 ratings\n",
    "glove_val_labels = to_categorical(glove_val_labels, num_classes=5) # number of classes, 1-5 ratings\n",
    "glove_test_labels = to_categorical(glove_test_labels, num_classes=5) # number of classes, 1-5 ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((1200, 100), (1200, 5), (400, 5), (400, 5))"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "data.shape, glove_train_labels.shape, glove_val_labels.shape, glove_test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove100_embedding_matrix = np.zeros((len(word_index) + 1, ndim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = glove100_embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        glove100_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "glove100_embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            ndim,\n",
    "                            weights=[glove100_embedding_matrix],\n",
    "                            input_length=100,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence_input = Input(shape=(100,), dtype='int32')\n",
    "# embedded_sequences = glove100_embedding_layer(sequence_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ 0.062659,  0.026003,  1.122   , -0.84123 , -0.56907 ,  0.86677 ,\n        0.99235 , -0.19032 , -0.74226 , -0.29547 ,  0.037746, -0.71756 ,\n       -0.44891 , -0.65801 ,  0.17469 ,  0.50347 ,  0.21268 ,  1.0438  ,\n       -0.60034 ,  0.48838 ,  0.16955 , -0.070132, -0.070228,  0.50193 ,\n        1.3543  , -0.27193 , -0.16449 ,  0.37399 , -0.14291 ,  0.19646 ,\n       -0.80925 ,  0.86245 , -0.2118  , -0.074321,  0.38522 , -0.34137 ,\n       -1.2694  ,  0.59967 , -0.84746 , -0.68818 ,  0.68874 , -0.19769 ,\n        0.2064  , -0.20387 , -0.33651 ,  0.093972,  0.34192 , -0.68139 ,\n        0.61451 , -0.88127 , -0.13401 , -0.088989, -0.098916,  0.73405 ,\n        0.33496 , -3.1068  , -0.22643 , -0.21568 ,  1.3375  ,  1.5472  ,\n       -1.2343  ,  0.42964 , -0.78932 , -0.027462,  0.34738 ,  0.17338 ,\n        0.24975 , -0.010451, -0.71149 ,  0.55568 , -0.20869 ,  0.074445,\n        0.29833 , -0.029668, -0.01595 ,  0.46368 , -0.80873 ,  0.11564 ,\n       -0.14607 ,  0.73785 ,  0.33548 ,  0.18176 , -1.2688  ,  0.21146 ,\n       -1.5877  ,  0.10299 , -0.55312 , -0.12812 , -0.035095,  0.12023 ,\n       -0.40742 ,  0.21107 , -0.4187  ,  0.12999 , -0.16549 , -0.55023 ,\n       -0.61101 ,  0.46126 ,  0.88781 , -0.031292], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "glove100_embedding_layer.get_weights()[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export weights\n",
    "#np.save('data_files/glove100_weights.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove 200 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 400000 word vectors.\n"
    }
   ],
   "source": [
    "glove200_path = 'glove.6B/glove.6B.200d.txt'\n",
    "glove200_embeddings_index = {}\n",
    "with open(glove200_path) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        glove200_embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(glove200_embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list()\n",
    "tokenizer = Tokenizer(nb_words = ndim)\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "sequences = tokenizer.texts_to_sequences(train_docs)\n",
    "data = pad_sequences(sequences, maxlen=ndim)\n",
    "word_index = tokenizer.word_index\n",
    "len(word_index)\n",
    "\n",
    "glove200_test = tokenizer.texts_to_sequences(test_docs)\n",
    "glove200_val = tokenizer.texts_to_sequences(val_docs)\n",
    "\n",
    "# Reuse glove train/val/test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "4200\n"
    }
   ],
   "source": [
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove200_embedding_matrix = np.zeros((len(word_index) + 1, ndim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = glove200_embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        glove200_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_200_embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            ndim,\n",
    "                            weights=[glove200_embedding_matrix],\n",
    "                            input_length=100,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove_100_embedding_layer and glove_200_embedding_layer are now layers that go in as training layers."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit0c31753cb4904f759510df829f98c315",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}